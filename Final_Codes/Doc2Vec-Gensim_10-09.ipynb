{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "np.random.seed(0)\n",
    "from pandas.io.json import json_normalize\n",
    "import io\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn import cross_validation\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import pickle\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   stars                                               text\n",
      "0      1  Super simple place but amazing nonetheless. It...\n",
      "1      1  Small unassuming place that changes their menu...\n",
      "2      1  Lester's is located in a beautiful neighborhoo...\n",
      "3      1  Love coming here. Yes the place always needs t...\n",
      "4      1  Had their chocolate almond croissant and it wa...\n",
      "(296227, 2)\n"
     ]
    }
   ],
   "source": [
    "# create the s3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# this is the location of the data on S3 (usual)\n",
    "bucket='yelpreviewsdata' # put your S3 bucket name here\n",
    "prefix = 'data'\n",
    "obj = s3.get_object(Bucket=bucket, Key=f'{prefix}/yelp_review-0000.csv')\n",
    "df = pd.read_csv(io.BytesIO(obj['Body'].read()), encoding=\"iso-8859-15\", low_memory=True)\n",
    "df = df[['stars', 'text']]\n",
    "df['stars']= np.where(df['stars'] >=2, 1,0)\n",
    "print(df.head())\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  stars                                               text\n",
      "0     1  The VIP has one of the best chicken wings in t...\n",
      "1     1  We so wanted this new restaurant and brew hous...\n",
      "2     1  I wasn't sure if I'd like Pilates on the refor...\n",
      "3     1  This place is awesome. Lots of great rolls at ...\n",
      "4     1  Super delicious Hainanese chicken and rice joi...\n",
      "(300000, 2)\n"
     ]
    }
   ],
   "source": [
    "def sampling_dataset(df):\n",
    "    count = 150000\n",
    "    class_df_sampled = pd.DataFrame(columns = [\"stars\",\"text\"])\n",
    "    temp = []\n",
    "    for c in df.stars.unique():\n",
    "        class_indexes = df[df.stars == c].index\n",
    "        random_indexes = np.random.choice(class_indexes, count, replace=True)\n",
    "        temp.append(df.loc[random_indexes])\n",
    "        \n",
    "    for each_df in temp:\n",
    "        class_df_sampled = pd.concat([class_df_sampled,each_df],axis=0)\n",
    "    \n",
    "    return class_df_sampled\n",
    "\n",
    "df = sampling_dataset(df)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "print (df.head())\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py:566: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py:570: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "w = re.compile(\"\\w+\",re.I)\n",
    "\n",
    "def label_sentences(df):\n",
    "    labeled_sentences = []\n",
    "    for index, datapoint in df.iterrows():\n",
    "        tokenized_words = re.findall(w,datapoint[\"text\"].lower())\n",
    "        labeled_sentences.append(LabeledSentence(words=tokenized_words, tags=['SENT_%s' %index]))\n",
    "    return labeled_sentences\n",
    "\n",
    "def train_doc2vec_model(labeled_sentences):\n",
    "    model = gensim.models.Doc2Vec(size=300, window=10, min_count=5, workers=11,alpha=0.025, min_alpha=0.025, iter=20)\n",
    "    model.build_vocab(labeled_sentences)\n",
    "    model.train(labeled_sentences, epochs=model.iter, total_examples=model.corpus_count)\n",
    "    return model\n",
    "\n",
    "sen = label_sentences(df)\n",
    "model = train_doc2vec_model(sen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  stars                                               text  \\\n",
      "0     1  The VIP has one of the best chicken wings in t...   \n",
      "1     1  We so wanted this new restaurant and brew hous...   \n",
      "2     1  I wasn't sure if I'd like Pilates on the refor...   \n",
      "3     1  This place is awesome. Lots of great rolls at ...   \n",
      "4     1  Super delicious Hainanese chicken and rice joi...   \n",
      "\n",
      "                                 vectorized_comments  \n",
      "0  [-0.33499905, 0.4270599, 0.87942874, 0.2583504...  \n",
      "1  [-0.69817674, 0.70658445, 0.86826485, -0.21861...  \n",
      "2  [-0.9702267, 1.2050159, 0.99320436, 0.56507194...  \n",
      "3  [-0.8135402, 0.7071663, -0.09651267, 0.3303958...  \n",
      "4  [-0.33375084, 0.29596585, 1.4535037, -0.216465...  \n"
     ]
    }
   ],
   "source": [
    "def vectorize_comments(df,d2v_model):\n",
    "    y = []\n",
    "    comments = []\n",
    "    for i in range(0,df.shape[0]):\n",
    "        label = 'SENT_%s' %i\n",
    "        comments.append(d2v_model.docvecs[label])\n",
    "    df['vectorized_comments'] = comments\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = vectorize_comments(df,model)\n",
    "print (df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(df[\"vectorized_comments\"].T.tolist(), df[\"stars\"], test_size=0.02, random_state=17)\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')\n",
    "def train_classifier(X,y):\n",
    "    n_estimators = [200,400]\n",
    "    min_samples_split = [2]\n",
    "    min_samples_leaf = [1]\n",
    "    bootstrap = [True]\n",
    "\n",
    "    parameters = {'n_estimators': n_estimators, 'min_samples_leaf': min_samples_leaf,\n",
    "                  'min_samples_split': min_samples_split}\n",
    "\n",
    "    clf = GridSearchCV(RFC(verbose=1,n_jobs=4), cv=4, param_grid=parameters)\n",
    "    clf.fit(X, y)\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV examined 30 candidate parameter settings.\n",
      "Model with rank: 1\n",
      "Mean OOB score: 0.584 (std: 0.002)\n",
      "Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 3, 'min_samples_leaf': 1}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean OOB score: 0.580 (std: 0.001)\n",
      "Parameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 3, 'min_samples_leaf': 1}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean OOB score: 0.579 (std: 0.001)\n",
      "Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 2, 'min_samples_leaf': 1}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean OOB score: 0.579 (std: 0.002)\n",
      "Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 3, 'min_samples_leaf': 4}\n",
      "\n",
      "Model with rank: 5\n",
      "Mean OOB score: 0.577 (std: 0.003)\n",
      "Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 2, 'min_samples_leaf': 1}\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-efeae3325f25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'entropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'None'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_weight_fraction_leaf\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"min_weight_fraction_leaf must in [0, 0.5]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_depth must be greater than zero. \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<=' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "#Randomized search for model selection\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=5):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean OOB score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 4),\n",
    "              \"min_samples_leaf\": sp_randint(1, 5),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 30\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "#this might take a minute to run\n",
    "print(\"RandomizedSearchCV examined %d candidate parameter settings.\" % (n_iter_search))\n",
    "report(random_search.cv_results_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "confusion matrix\n",
      "Predicted     0     1   All\n",
      "True                       \n",
      "0          1817  1197  3014\n",
      "1          1230  1756  2986\n",
      "All        3047  2953  6000\n",
      "\n",
      "auc score 0.5954655234714\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_features = 3, criterion = 'entropy', min_samples_leaf = 1)\n",
    "\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "y_pred=clf.predict(X_test)\n",
    "y_scores=clf.predict_proba(X_test)\n",
    "print ('\\nconfusion matrix')\n",
    "print(pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "print('\\nauc score '+str(auc(false_positive_rate, true_positive_rate)))\n",
    "\n",
    "#show a tradeoff curve for precision vs recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7045"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(preds == y_test) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
